---
title: "知识蒸馏" #标题
date: "2025-08-16" #创建时间
lastmod: "2025-08-16"
author: ["Plutoxx28"] #作者
categories: 
- AI
tags: 
- concept
description: "从概念原理到实践应用的系统梳理" #文章描述
weight: # 输入1可以顶置文章，用来给文章展示排序，不填就默认按时间排序
slug: "knowledge-distillation" #seo使用，示例：http://example.com/ultimate-guide-making-perfect-pasta
draft: false # 是否为草稿
comments: true #是否展示评论
ShowToc: true # 显示目录
TocOpen: true # 自动展开目录
hidemeta: false # 是否隐藏文章的元信息，如发布日期、作者等
disableShare: true # 底部不显示分享栏
ShowBreadCrumbs: true #顶部显示当前路径
ShowReadingTime: true #展示阅读时间
cover:
    image: "" #图片路径：posts/tech/文章1/picture.png
    caption: "" #图片底部描述
    alt: "" #这里填写图片无法显示时的替代文本
    relative: false # 如果这个路径是从网站根目录开始的，这里就保持为 false
---


知识蒸馏（Knowledge Distillation, KD）是一类把强模型能力迁移到弱模型的技术集合，常用于模型压缩、加速推理与降低部署成本。从 Hinton 等人在 2015 年系统化提出"软目标（soft targets）蒸馏"开始，到大语言模型时代流行的"用强模型生成高质量训练数据再训练弱模型"，蒸馏的"监督信号形态"与"实现路径"都在扩展。本文按概念与技术维度梳理蒸馏的主流做法，并用 DeepSeek-R1 的蒸馏实践做具体化说明。


## 一、先把术语摆正：蒸馏的两条主线

讨论蒸馏时最容易混淆的一点是：把"监督信号的类型"与"蒸馏方法的类别"混为一谈。更清晰的切法是先区分两条主线：

### 1. 分布匹配型蒸馏（distribution-matching KD）

教师模型提供的是"概率分布 / logits（可带温度）"，学生模型通过 KL/CE 去对齐教师分布。这是 Hinton 2015 经典蒸馏范式。

### 2. 数据蒸馏 / 序列蒸馏（data distillation / sequence or trajectory distillation）

教师模型提供的是"生成的文本序列、推理轨迹（例如 CoT）、工具调用轨迹等"，学生模型用这些数据做监督微调（SFT）或类似的 token-level 最大似然训练。这里更准确的称呼是"伪标签（pseudo-label）训练/序列蒸馏"，而不是"硬标签蒸馏"。

> **小结**：这两条主线都属于"蒸馏"的范畴，但监督信号与训练机制不同：前者显式匹配分布，后者通过高质量样本间接迁移能力。


## 二、软标签、硬标签与伪标签：不要混用

### 1. 硬标签（Hard Labels）

硬标签通常指来自人工标注或规则生成的离散标签/答案：

- one-hot 分类标签
- 二分类 0/1
- 多标签 0/1 向量等

它表达的是"唯一正确答案"，不包含不确定性与类别相似性信息。

### 2. 软标签（Soft Labels / Soft Targets）

软标签通常指教师模型对类别的概率分布（或温度缩放后的概率分布），例如：

```
[0.10, 0.05, 0.70, 0.10, 0.05]
```

它不仅告诉学生"最可能是第 3 类"，还隐含了"其他类与目标类的相对相似性"。Hinton 把这类额外信息称为**暗知识（Dark Knowledge）**。

### 3. 伪标签（Pseudo-labels）/ 序列监督

在大语言模型场景里，"教师输出的完整文本答案、CoT 推理过程、步骤化解法、工具调用轨迹"等更适合称为伪标签或轨迹数据。它们是"由教师生成的监督样本"，不是传统意义的硬标签。

> **结论**：软/硬标签是监督信号类型；序列/轨迹是另一类监督信号形态。不要把"教师生成的确定文本"直接叫硬标签。


## 三、经典知识蒸馏原理：分布匹配（Hinton 2015）

经典蒸馏的核心是：让学生模型的输出分布逼近教师模型的输出分布。常见做法是把"蒸馏损失（soft）"与"真实标签损失（hard）"加权求和。

### 1. 温度缩放与 soft targets

给定 logits \( z \)，温度 \( T \) 的 softmax：

\[
p_T = \text{softmax}(z / T)
\]

- \( T \) 越大，分布越平滑
- \( T \) 越小，分布越尖锐

蒸馏常用较大的 \( T \) 来暴露"非最高类"的相对概率，从而传递暗知识。

### 2. 损失函数（典型写法）

设：

- 教师分布：\( p_T^{\text{teacher}} \)
- 学生分布：\( p_T^{\text{student}} \)
- 真实标签：\( y \)（hard label）

**蒸馏损失（soft）**：

\[
\mathcal{L}_{\text{soft}} = \text{KL}(p_T^{\text{teacher}} \| p_T^{\text{student}})
\]

**标准监督损失（hard，可选）**：

\[
\mathcal{L}_{\text{hard}} = \text{CE}(y, p_1^{\text{student}})
\]

**总损失**（关键点：soft 项通常乘 \( T^2 \) 以保持梯度量级一致）：

\[
\mathcal{L}_{\text{total}} = \alpha \cdot T^2 \cdot \mathcal{L}_{\text{soft}} + (1-\alpha) \cdot \mathcal{L}_{\text{hard}}
\]

> **要点**：当 \( T \) 改变时，梯度会随 \( 1/T^2 \) 缩放，乘回 \( T^2 \) 是为了让 \( \alpha \) 的含义稳定（否则 \( T \) 变大时 soft 部分影响会被意外削弱）。


## 四、蒸馏方法的技术维度：按"能拿到什么 + 学什么"来分类

比起"传统/现代"这类时间分法，更实用的是拆成可操作维度。

### （一）按可访问性：白盒 vs 黑盒

| 类型 | 描述 | 优缺点 |
|------|------|--------|
| **白盒蒸馏（White-box）** | 可访问教师的权重、中间层表示、注意力矩阵等 | ✅ 监督信号更丰富（可做特征/注意力对齐）<br>❌ 需要完整控制教师模型 |
| **黑盒蒸馏（Black-box）** | 只能通过 API 获得输出（文本或概率/logprobs） | ✅ 适合闭源教师或商业 API 场景<br>常见实现："输出分布对齐"或"数据蒸馏" |

### （二）按学习目标：学中间表示 vs 学最终行为

#### 1. 表示/特征匹配（Feature / Representation-based）

学生对齐教师的中间层隐藏状态、特征图、注意力等，例如：

```python
teacher_hidden = teacher.layer_n(x)
student_hidden = student.layer_m(x)
loss = MSE(project(student_hidden), teacher_hidden)
```

这里常需要投影层（`project`）解决维度不一致；也经常需要结构或层级映射。

#### 2. 输出/行为匹配（Output / Behavior-based）

- **分类任务**：对齐 teacher 的 soft targets（KL/CE + T）
- **生成任务**：对齐 teacher 生成的 token 序列（token-level NLL/CE），或对齐工具调用轨迹等行为序列

> **注**：Hinton 2015 的经典蒸馏主要是"最终输出分布（soft targets）对齐"，不要求中间层匹配。

### （三）按监督信号形态：logits/probs vs tokens/trajectories

| 类型 | 描述 |
|------|------|
| **分布蒸馏（logits/probs）** | 学生学习"在多个候选之间如何分配概率"，本质是匹配教师不确定性与类间关系 |
| **序列/轨迹蒸馏（tokens/trajectories）** | 学生学习"可复现的解题轨迹"：答案文本、CoT、分步推理、工具调用序列等。本质是用教师生成数据把"能力"封装进训练集，再通过 SFT 抽取出来 |


## 五、DeepSeek-R1 蒸馏：序列/轨迹数据驱动的蒸馏案例

DeepSeek-R1 的蒸馏实践可以视为典型的"数据蒸馏/序列蒸馏"路径：强教师生成高质量推理与通用任务数据，经过筛选与清洗后，用于微调多个开源学生底座模型。

### 1. 数据侧：生成与筛选

| 数据类型 | 规模 | 说明 |
|----------|------|------|
| **推理相关数据** | ~600k | 通过采样与拒绝采样（rejection sampling）生成大量候选，再做质量筛选 |
| **非推理数据** | ~200k | 通用任务数据（写作、事实问答、翻译等），用于保持模型通用能力与稳定性 |
| **合计** | ~800k | 精选训练集，包含较严格的质量控制流程 |

### 2. 训练侧：以 SFT 为主

- 蒸馏出的学生模型通常采用"仅监督微调（SFT only）"的方式训练
- 不包含教师侧可能用到的强化学习（RL）阶段
- 学生底座可以是不同家族（例如 Qwen、Llama），无需结构对齐

> 这也是黑盒/数据蒸馏路径的典型优势：不依赖教师内部表示，只依赖"高质量轨迹数据"。

### 3. 经验结论：小模型直接做大规模 RL 不一定划算

论文对比表明：同等规模下，基于高质量蒸馏数据做 SFT 的学生模型，性能可以显著超过"在同尺寸模型上直接做 RL（R1-Zero 路线）"的结果。

> **启示**：对小底座而言，先用强教师产出高质量轨迹数据再蒸馏，往往更经济有效。


## 六、把"教学产物"与"学生学到的东西"对齐

理解蒸馏可以用一个统一视角：**教师提供什么监督信号，学生就主要学到什么"可迁移结构"**。

### 1. 教师给分布（logits/probs）

学生学到的是"细粒度的不确定性结构"：

- 哪些候选接近
- 哪些候选相互排斥
- 置信度如何分配

它更像在学习"判断边界与相似性关系"。

### 2. 教师给轨迹（序列/CoT/工具调用）

学生学到的是"可复现的解题轨迹模板与操作序列"。这不是简单的"填入最终答案"，而是把"过程"显式写进监督信号里，让学生在 token 级别拟合这些过程，从而内化为可泛化的推理习惯与策略。

> **需要强调**：序列/轨迹蒸馏并不等同于严格的分布匹配；它更像是对教师行为分布的一种"采样近似"，用足够多、足够高质量的轨迹覆盖任务空间来逼近能力迁移。


## 七、商业应用价值：蒸馏的三类可兑现资产

### 1. 降本增效

把昂贵教师的能力迁移到更小模型上，降低推理成本与延迟，提升可部署性。

### 2. 能力门槛下沉

大量团队无法训练顶级教师，但可以购买/调用教师能力，产出蒸馏数据训练可控的私有学生模型。

### 3. 数据增值

把原始数据升级为"知识增强数据集"：包含推理、步骤、工具调用与质量筛选的高密度监督信号，使训练集本身成为资产。

**潜在商业模式**：

- 教师生成数据集服务
- 蒸馏即服务（DaaS）
- 面向行业的模型压缩与私有化落地方案


## 参考资料

1. Hinton, G., Vinyals, O., & Dean, J. (2015). *Distilling the Knowledge in a Neural Network*. arXiv:1503.02531.
2. Romero, A., et al. (2014). *FitNets: Hints for Thin Deep Nets*. arXiv:1412.6550.
3. Zagoruyko, S., & Komodakis, N. (2016). *Attention Transfer*. arXiv:1612.03928.
4. Gou, J., et al. (2021). *Knowledge Distillation: A Survey*. IJCV, 129(6), 1789-1819.
5. DeepSeek AI. (2025). *DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning*. arXiv:2501.12948.

