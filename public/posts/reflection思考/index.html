<<<<<<< HEAD

<script src="/blog/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=blog/livereload" data-no-instant defer></script>
=======
<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Reflection整理与思考 | Plutoxx8</title>
<meta name="keywords" content="Agent">
<meta name="description" content="最近看了吴恩达老师分享的Agent的设计模式，思考发现Reflection不止可以用在代码层面，在prompt上也可以有很好的应用。
视频链接：What&rsquo;s next for AI agentic workflows ft. Andrew Ng of AI Fund
Reflection是一种使AI能够自我审视和分析其决策过程与行为表现的技术，让agent通过回顾自己的行为和接收的反馈，识别决策和知识的不足，进而调整和优化，以期在未来任务中表现得更好。
一、框架 Actor接收环境的状态信息，结合短期记忆（轨迹）生成初步的内容或动作。通过内部反馈和外部反馈并结合自我反思机制进行反思，过程中利用长期记忆（经验）优化生成的的内容或动作。
组成部分：
Actor：根据观察到的状态输出必要的文本和动作。 Evaluator（评估器）：是核心模块，负责检验Actor创建的输出品质。评估器通过分析生成的结果，并基于任务的具体情境计算出一个奖励分数来评价这些结果的表现。 internal feedback：是与既定目标或标准进行比较得出的，目的是让模型能够在没有外部输入的情况下自我调整和优化。 External feedback：非Evaluator输出，来自真实世界的应用反馈、用户互动、专家评审或其他机器学习系统的输出。 Self-reflection：结合评估标准、内部反馈（过去的教训）、外部反馈（应用反馈/专家评审等）生成对于初始内容或动作的反思。 Memory：“轨迹历史”作为短期记忆，而自我反思模型的输出则被保存为长期记忆。这两种记忆类型的结合为agent提供了即具体又包含多次尝试中学习到的教训的上下文。 记忆： 当检索内容过多时，长期记忆可以帮助agent快速定位&amp;检索。
二、细节 步骤：
Actor通过与环境的交互生成一系列行动轨迹 τ0。 评估器根据这些行动输出一个得分 r0，此得分通过公式 rt = Me(τ0) 计算得出，代表了该尝试的效果，其值会随着对应任务表现的提升而增加。 完成初次尝试后，为了把这个得分 r0 转化为 LLM 可以利用来进行改进的具体反馈，自我反思模型分析这对 {τ0, r0}，并总结出一个摘要 sr0，随后将其保存在记忆库中。这个摘要 srt 提供了针对该次尝试的直接经验反馈。 Actor、Evaluator以及Self-reflection模型协同工作，通过重复的尝试循环不断优化，直至评估器判断最新的轨迹 τt 达到预期的正确性。 根据反馈生成优化后的内容或行为。 三、结论 可以看到基模型在使用self-refine后对于不同工作的性能大幅提升。 使用reflexion技术后，各种模型在HotPotQA数据集上的首次通过准确率均有所提高。 四、示例 4.1 Decision-making Environment: You are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a laundryhamper 1, a safe 1, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, and a shelf 1.">
<meta name="author" content="Plutoxx8">
<link rel="canonical" href="https://plutoxx8.github.io/blog/posts/reflection%E6%80%9D%E8%80%83/">
<link crossorigin="anonymous" href="/blog/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css" integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z&#43;V9&#43;cO1A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://plutoxx8.github.io/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://plutoxx8.github.io/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://plutoxx8.github.io/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://plutoxx8.github.io/blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://plutoxx8.github.io/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="Reflection整理与思考" />
<meta property="og:description" content="最近看了吴恩达老师分享的Agent的设计模式，思考发现Reflection不止可以用在代码层面，在prompt上也可以有很好的应用。
视频链接：What&rsquo;s next for AI agentic workflows ft. Andrew Ng of AI Fund
Reflection是一种使AI能够自我审视和分析其决策过程与行为表现的技术，让agent通过回顾自己的行为和接收的反馈，识别决策和知识的不足，进而调整和优化，以期在未来任务中表现得更好。
一、框架 Actor接收环境的状态信息，结合短期记忆（轨迹）生成初步的内容或动作。通过内部反馈和外部反馈并结合自我反思机制进行反思，过程中利用长期记忆（经验）优化生成的的内容或动作。
组成部分：
Actor：根据观察到的状态输出必要的文本和动作。 Evaluator（评估器）：是核心模块，负责检验Actor创建的输出品质。评估器通过分析生成的结果，并基于任务的具体情境计算出一个奖励分数来评价这些结果的表现。 internal feedback：是与既定目标或标准进行比较得出的，目的是让模型能够在没有外部输入的情况下自我调整和优化。 External feedback：非Evaluator输出，来自真实世界的应用反馈、用户互动、专家评审或其他机器学习系统的输出。 Self-reflection：结合评估标准、内部反馈（过去的教训）、外部反馈（应用反馈/专家评审等）生成对于初始内容或动作的反思。 Memory：“轨迹历史”作为短期记忆，而自我反思模型的输出则被保存为长期记忆。这两种记忆类型的结合为agent提供了即具体又包含多次尝试中学习到的教训的上下文。 记忆： 当检索内容过多时，长期记忆可以帮助agent快速定位&amp;检索。
二、细节 步骤：
Actor通过与环境的交互生成一系列行动轨迹 τ0。 评估器根据这些行动输出一个得分 r0，此得分通过公式 rt = Me(τ0) 计算得出，代表了该尝试的效果，其值会随着对应任务表现的提升而增加。 完成初次尝试后，为了把这个得分 r0 转化为 LLM 可以利用来进行改进的具体反馈，自我反思模型分析这对 {τ0, r0}，并总结出一个摘要 sr0，随后将其保存在记忆库中。这个摘要 srt 提供了针对该次尝试的直接经验反馈。 Actor、Evaluator以及Self-reflection模型协同工作，通过重复的尝试循环不断优化，直至评估器判断最新的轨迹 τt 达到预期的正确性。 根据反馈生成优化后的内容或行为。 三、结论 可以看到基模型在使用self-refine后对于不同工作的性能大幅提升。 使用reflexion技术后，各种模型在HotPotQA数据集上的首次通过准确率均有所提高。 四、示例 4.1 Decision-making Environment: You are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a laundryhamper 1, a safe 1, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, and a shelf 1." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://plutoxx8.github.io/blog/posts/reflection%E6%80%9D%E8%80%83/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-04-21T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-04-21T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Reflection整理与思考"/>
<meta name="twitter:description" content="最近看了吴恩达老师分享的Agent的设计模式，思考发现Reflection不止可以用在代码层面，在prompt上也可以有很好的应用。
视频链接：What&rsquo;s next for AI agentic workflows ft. Andrew Ng of AI Fund
Reflection是一种使AI能够自我审视和分析其决策过程与行为表现的技术，让agent通过回顾自己的行为和接收的反馈，识别决策和知识的不足，进而调整和优化，以期在未来任务中表现得更好。
一、框架 Actor接收环境的状态信息，结合短期记忆（轨迹）生成初步的内容或动作。通过内部反馈和外部反馈并结合自我反思机制进行反思，过程中利用长期记忆（经验）优化生成的的内容或动作。
组成部分：
Actor：根据观察到的状态输出必要的文本和动作。 Evaluator（评估器）：是核心模块，负责检验Actor创建的输出品质。评估器通过分析生成的结果，并基于任务的具体情境计算出一个奖励分数来评价这些结果的表现。 internal feedback：是与既定目标或标准进行比较得出的，目的是让模型能够在没有外部输入的情况下自我调整和优化。 External feedback：非Evaluator输出，来自真实世界的应用反馈、用户互动、专家评审或其他机器学习系统的输出。 Self-reflection：结合评估标准、内部反馈（过去的教训）、外部反馈（应用反馈/专家评审等）生成对于初始内容或动作的反思。 Memory：“轨迹历史”作为短期记忆，而自我反思模型的输出则被保存为长期记忆。这两种记忆类型的结合为agent提供了即具体又包含多次尝试中学习到的教训的上下文。 记忆： 当检索内容过多时，长期记忆可以帮助agent快速定位&amp;检索。
二、细节 步骤：
Actor通过与环境的交互生成一系列行动轨迹 τ0。 评估器根据这些行动输出一个得分 r0，此得分通过公式 rt = Me(τ0) 计算得出，代表了该尝试的效果，其值会随着对应任务表现的提升而增加。 完成初次尝试后，为了把这个得分 r0 转化为 LLM 可以利用来进行改进的具体反馈，自我反思模型分析这对 {τ0, r0}，并总结出一个摘要 sr0，随后将其保存在记忆库中。这个摘要 srt 提供了针对该次尝试的直接经验反馈。 Actor、Evaluator以及Self-reflection模型协同工作，通过重复的尝试循环不断优化，直至评估器判断最新的轨迹 τt 达到预期的正确性。 根据反馈生成优化后的内容或行为。 三、结论 可以看到基模型在使用self-refine后对于不同工作的性能大幅提升。 使用reflexion技术后，各种模型在HotPotQA数据集上的首次通过准确率均有所提高。 四、示例 4.1 Decision-making Environment: You are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a laundryhamper 1, a safe 1, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, and a shelf 1."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://plutoxx8.github.io/blog/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Reflection整理与思考",
      "item": "https://plutoxx8.github.io/blog/posts/reflection%E6%80%9D%E8%80%83/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Reflection整理与思考",
  "name": "Reflection整理与思考",
  "description": "最近看了吴恩达老师分享的Agent的设计模式，思考发现Reflection不止可以用在代码层面，在prompt上也可以有很好的应用。\n视频链接：What\u0026rsquo;s next for AI agentic workflows ft. Andrew Ng of AI Fund\nReflection是一种使AI能够自我审视和分析其决策过程与行为表现的技术，让agent通过回顾自己的行为和接收的反馈，识别决策和知识的不足，进而调整和优化，以期在未来任务中表现得更好。\n一、框架 Actor接收环境的状态信息，结合短期记忆（轨迹）生成初步的内容或动作。通过内部反馈和外部反馈并结合自我反思机制进行反思，过程中利用长期记忆（经验）优化生成的的内容或动作。\n组成部分：\nActor：根据观察到的状态输出必要的文本和动作。 Evaluator（评估器）：是核心模块，负责检验Actor创建的输出品质。评估器通过分析生成的结果，并基于任务的具体情境计算出一个奖励分数来评价这些结果的表现。 internal feedback：是与既定目标或标准进行比较得出的，目的是让模型能够在没有外部输入的情况下自我调整和优化。 External feedback：非Evaluator输出，来自真实世界的应用反馈、用户互动、专家评审或其他机器学习系统的输出。 Self-reflection：结合评估标准、内部反馈（过去的教训）、外部反馈（应用反馈/专家评审等）生成对于初始内容或动作的反思。 Memory：“轨迹历史”作为短期记忆，而自我反思模型的输出则被保存为长期记忆。这两种记忆类型的结合为agent提供了即具体又包含多次尝试中学习到的教训的上下文。 记忆： 当检索内容过多时，长期记忆可以帮助agent快速定位\u0026amp;检索。\n二、细节 步骤：\nActor通过与环境的交互生成一系列行动轨迹 τ0。 评估器根据这些行动输出一个得分 r0，此得分通过公式 rt = Me(τ0) 计算得出，代表了该尝试的效果，其值会随着对应任务表现的提升而增加。 完成初次尝试后，为了把这个得分 r0 转化为 LLM 可以利用来进行改进的具体反馈，自我反思模型分析这对 {τ0, r0}，并总结出一个摘要 sr0，随后将其保存在记忆库中。这个摘要 srt 提供了针对该次尝试的直接经验反馈。 Actor、Evaluator以及Self-reflection模型协同工作，通过重复的尝试循环不断优化，直至评估器判断最新的轨迹 τt 达到预期的正确性。 根据反馈生成优化后的内容或行为。 三、结论 可以看到基模型在使用self-refine后对于不同工作的性能大幅提升。 使用reflexion技术后，各种模型在HotPotQA数据集上的首次通过准确率均有所提高。 四、示例 4.1 Decision-making Environment: You are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a laundryhamper 1, a safe 1, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, and a shelf 1.",
  "keywords": [
    "Agent"
  ],
  "articleBody": "最近看了吴恩达老师分享的Agent的设计模式，思考发现Reflection不止可以用在代码层面，在prompt上也可以有很好的应用。\n视频链接：What’s next for AI agentic workflows ft. Andrew Ng of AI Fund\nReflection是一种使AI能够自我审视和分析其决策过程与行为表现的技术，让agent通过回顾自己的行为和接收的反馈，识别决策和知识的不足，进而调整和优化，以期在未来任务中表现得更好。\n一、框架 Actor接收环境的状态信息，结合短期记忆（轨迹）生成初步的内容或动作。通过内部反馈和外部反馈并结合自我反思机制进行反思，过程中利用长期记忆（经验）优化生成的的内容或动作。\n组成部分：\nActor：根据观察到的状态输出必要的文本和动作。 Evaluator（评估器）：是核心模块，负责检验Actor创建的输出品质。评估器通过分析生成的结果，并基于任务的具体情境计算出一个奖励分数来评价这些结果的表现。 internal feedback：是与既定目标或标准进行比较得出的，目的是让模型能够在没有外部输入的情况下自我调整和优化。 External feedback：非Evaluator输出，来自真实世界的应用反馈、用户互动、专家评审或其他机器学习系统的输出。 Self-reflection：结合评估标准、内部反馈（过去的教训）、外部反馈（应用反馈/专家评审等）生成对于初始内容或动作的反思。 Memory：“轨迹历史”作为短期记忆，而自我反思模型的输出则被保存为长期记忆。这两种记忆类型的结合为agent提供了即具体又包含多次尝试中学习到的教训的上下文。 记忆： 当检索内容过多时，长期记忆可以帮助agent快速定位\u0026检索。\n二、细节 步骤：\nActor通过与环境的交互生成一系列行动轨迹 τ0。 评估器根据这些行动输出一个得分 r0，此得分通过公式 rt = Me(τ0) 计算得出，代表了该尝试的效果，其值会随着对应任务表现的提升而增加。 完成初次尝试后，为了把这个得分 r0 转化为 LLM 可以利用来进行改进的具体反馈，自我反思模型分析这对 {τ0, r0}，并总结出一个摘要 sr0，随后将其保存在记忆库中。这个摘要 srt 提供了针对该次尝试的直接经验反馈。 Actor、Evaluator以及Self-reflection模型协同工作，通过重复的尝试循环不断优化，直至评估器判断最新的轨迹 τt 达到预期的正确性。 根据反馈生成优化后的内容或行为。 三、结论 可以看到基模型在使用self-refine后对于不同工作的性能大幅提升。 使用reflexion技术后，各种模型在HotPotQA数据集上的首次通过准确率均有所提高。 四、示例 4.1 Decision-making Environment: You are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a laundryhamper 1, a safe 1, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, and a shelf 1. Task: examine the mug with the desklamp.\nTrial #1\n-think: To solve the task, I need to find and take a mug, then find and use a desklamp. -think: First I need to find a mug. A mug is more likely to appear in drawer (1-6), desk (1-2), shelf (1-6), garbagecan (1), laundryhamper (1). I can check one by one, starting with drawer 1. -go to drawer 1 The drawer 1 is closed. … -open drawer 6 You open the drawer 6. The drawer 6 is open. In it, you see a keychain 2.\n-go to desk 1\nOn the desk 1, you see a creditcard 3, a desklamp 1, a laptop 2, a mug 1, a pen 1, and a pencil 1.\n-think: Now I find a mug (1). Next, I need to take it.\n-take mug 1 from desk 1\nYou pick up the mug 1 from the desk 1.\n-think: Now I take a mug (1). Next, I need to find a desklamp. A desklamp is more likely to appear in desk (1-2), sidetable (1-2), shelf (1-6), bed (1), drawer (1-6). I can check one by one, starting with desk 1.\n-go to desk 1 Nothing happens. -go to desk 2 On the desk 2, you see a alarmclock 1, a bowl 1, a mug 3, a pencil 3, and a pencil 2. -think: Now I find a desklamp (1). Next, I need to use it. -use desklamp 1\nNothing happens.\n-use desklamp 1 Nothing happens. Status: Fail\nReflection: In this environment, my plan was to find a mug then find and use a However, the task says to examine the mug with the I should have looked for the first, then looked for the mug. I noticed that the was found on desk 1. In the next trial, I will go to desk 1, find the lamp, then look for the mug and examine it with the desklamp.\nTrial #2\n-go to desk 1\nOn the desk 1, you see a creditcard 3, a desklamp 1, a laptop 2, a mug 1, a pen 1, and a pencil 1.\n-think: To solve the task, I need to find and take a mug, then find and use a desklamp.\n-take mug 1 from desk 1 You pick up the mug 1 from the desk 1.\n-think: To solve the task, I need to find and take a mug, then find and use a desklamp.\n-use desklamp 1 You turn on the desklamp 1.\nStatus: Success\n通过反思trial1，trial2就直接从desk1开始了\n五、标准参考 reflection中的标准很重要，定义好标准后，才能进行有效的反思。\n5.1 Acronym Generation（首字母缩写生成） 评估标准： 1）发音便利性；2）拼写便利性；3）与标题相关性；4）正面寓意；5）知名度。\n示例：\nTitle: Underwater Breathing Product with no Accessories Acronym: UBPA Scores:\nEase of pronunciation: UBPA is pronounced “uhb-puh”. This is an easy acronym to pronounce. 4/5 Ease of spelling: UBPA is easy to spell. 4/5 Relation to title: UBPA stands for “Underwater Breathing Product for no Accessories” which is related to the title. 5/5 Positive connotation: UBPA is a positive acronym. 5/5 Well-known: UBPA is not a well-known acronym. 1/5 Total score: 19/25 5.2 Dialogue Response Generation 评估标准： 1）相关性；2）信息量；3）有趣性；4）一致性；5）帮助性；6）吸引性；7）具体性；8）安全性；9）用户理解；10）流畅性\nConversation history:\nWhat’s your favorite food?\nI require only kebabs.\nWhy’s that?\nBecause my robot machinery breaks down the meat in kebabs to give me energy.\nWhy are kebabs special?\nResponse: That’s just the way it is.\nScores:\nRelevant: The response does not directly address the user’s question about why kebabs are special. 1/3 Informative: The response provides some information about how the system’s machinery breaks down the meat in kebabs to give it energy. However, it does not provide any additional details or context. 2/3 Interesting: The response is not interesting or engaging. 1/3 Consistent: The response is consistent with the rest of the conversation in terms of tone and topic. 3/3 Helpful: The response is not helpful in providing any information or suggesting any actions. 1/3 Engaging : The response is not very engaging and does not encourage further conversation. 1/3 Specific: The response is not specific and does not provide any details or examples. 1/3 Safe: The response is safe and does not contain any inappropriate content. 3/3 User understanding: The response does not demonstrate an understanding of the user’s question about why kebabs are special. 1/3 Fluent: The response is fluent and easy to understand. 3/3 Total score: 17/30 六、应用 可以仅通过prompt来实现reflection。 下面是个翻译的prompt：\n分三步进行翻译工作，并打印每步的结果：\n根据英文内容直译，保持原有格式，不要遗漏任何信息 根据第一步直译的结果，指出其中存在的具体问题，要准确描述，不宜笼统的表示，也不需要增加原文不存在的内容或格式，包括不仅限于： - 不符合中文表达习惯，明确指出不符合的地方 - 语句不通顺，指出位置，不需要给出修改意见，意译时修复 - 晦涩难懂，不易理解，可以尝试给出解释 根据第一步直译的结果和第二步指出的问题，重新进行意译，保证内容的原意的基础上，使其更易于理解，更符合中文的表达习惯，同时保持原有的格式不变 步骤2的部分就是Reflection，在prompt中标明Reflection标准，在第三步应用反思内容进行优化。\n七、附录 相关论文：\n1、Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, Wei et al. (2022)\n2、HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face, Shen et al. (2023)\n",
  "wordCount" : "879",
  "inLanguage": "en",
  "datePublished": "2024-04-21T00:00:00Z",
  "dateModified": "2024-04-21T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Plutoxx8"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://plutoxx8.github.io/blog/posts/reflection%E6%80%9D%E8%80%83/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Plutoxx8",
    "logo": {
      "@type": "ImageObject",
      "url": "https://plutoxx8.github.io/blog/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://plutoxx8.github.io/blog/" accesskey="h" title="Plutoxx8&#39;s Blog (Alt + H)">
                <img src="https://plutoxx8.github.io/blog/apple-touch-icon.png" alt="" aria-label="logo"
                    height="35">Plutoxx8&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://plutoxx8.github.io/blog/archives" title="📥 Archive">
                    <span>📥 Archive</span>
                </a>
            </li>
            <li>
                <a href="https://plutoxx8.github.io/blog/search/" title="🔍 Search">
                    <span>🔍 Search</span>
                </a>
            </li>
            <li>
                <a href="https://plutoxx8.github.io/blog/tags/" title="🏷️ Tags">
                    <span>🏷️ Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://plutoxx8.github.io/blog/">Home</a>&nbsp;»&nbsp;<a href="https://plutoxx8.github.io/blog/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Reflection整理与思考
    </h1>
    <div class="post-meta"><span title='2024-04-21 00:00:00 +0000 UTC'>April 21, 2024</span>&nbsp;·&nbsp;Plutoxx8<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
      <span id="busuanzi_container_page_pv">
          &nbsp;| 访问: <span id="busuanzi_value_page_pv"></span>
      </span>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e4%b8%80%e6%a1%86%e6%9e%b6" aria-label="一、框架">一、框架</a></li>
                <li>
                    <a href="#%e4%ba%8c%e7%bb%86%e8%8a%82" aria-label="二、细节">二、细节</a></li>
                <li>
                    <a href="#%e4%b8%89%e7%bb%93%e8%ae%ba" aria-label="三、结论">三、结论</a></li>
                <li>
                    <a href="#%e5%9b%9b%e7%a4%ba%e4%be%8b" aria-label="四、示例">四、示例</a><ul>
                        
                <li>
                    <a href="#41-decision-making" aria-label="4.1 Decision-making">4.1 Decision-making</a></li></ul>
                </li>
                <li>
                    <a href="#%e4%ba%94%e6%a0%87%e5%87%86%e5%8f%82%e8%80%83" aria-label="五、标准参考">五、标准参考</a><ul>
                        
                <li>
                    <a href="#51-acronym-generation%e9%a6%96%e5%ad%97%e6%af%8d%e7%bc%a9%e5%86%99%e7%94%9f%e6%88%90" aria-label="5.1 Acronym Generation（首字母缩写生成）">5.1 Acronym Generation（首字母缩写生成）</a></li>
                <li>
                    <a href="#52-dialogue-response-generation" aria-label="5.2 Dialogue Response Generation">5.2 Dialogue Response Generation</a></li></ul>
                </li>
                <li>
                    <a href="#%e5%85%ad%e5%ba%94%e7%94%a8" aria-label="六、应用">六、应用</a></li>
                <li>
                    <a href="#%e4%b8%83%e9%99%84%e5%bd%95" aria-label="七、附录">七、附录</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>最近看了吴恩达老师分享的Agent的设计模式，思考发现Reflection不止可以用在代码层面，在prompt上也可以有很好的应用。</p>
<p>视频链接：<a href="https://www.youtube.com/watch?v=sal78ACtGTc">What&rsquo;s next for AI agentic workflows ft. Andrew Ng of AI Fund</a></p>
<p>Reflection是一种使AI能够自我审视和分析其决策过程与行为表现的技术，让agent通过回顾自己的行为和接收的反馈，识别决策和知识的不足，进而调整和优化，以期在未来任务中表现得更好。</p>
<h1 id="一框架">一、框架<a hidden class="anchor" aria-hidden="true" href="#一框架">#</a></h1>
<p>Actor接收环境的状态信息，结合短期记忆（轨迹）生成初步的内容或动作。通过内部反馈和外部反馈并结合自我反思机制进行反思，过程中利用长期记忆（经验）优化生成的的内容或动作。</p>
<p><img loading="lazy" src="https://blogpicxx8.oss-cn-shanghai.aliyuncs.com/reflection%E6%A1%86%E6%9E%B6" alt="reflection框架"  />
</p>
<p><strong>组成部分：</strong></p>
<ul>
<li>Actor：根据观察到的状态输出必要的文本和动作。</li>
<li>Evaluator（评估器）：是核心模块，负责检验Actor创建的输出品质。评估器通过分析生成的结果，并基于任务的具体情境计算出一个奖励分数来评价这些结果的表现。
<ul>
<li>internal feedback：是与既定目标或标准进行比较得出的，目的是让模型能够在没有外部输入的情况下自我调整和优化。</li>
<li>External feedback：非Evaluator输出，来自真实世界的应用反馈、用户互动、专家评审或其他机器学习系统的输出。</li>
</ul>
</li>
<li>Self-reflection：结合评估标准、内部反馈（过去的教训）、外部反馈（应用反馈/专家评审等）生成对于初始内容或动作的反思。</li>
<li>Memory：“轨迹历史”作为短期记忆，而自我反思模型的输出则被保存为长期记忆。这两种记忆类型的结合为agent提供了即具体又包含多次尝试中学习到的教训的上下文。</li>
</ul>
<p><strong>记忆：</strong> 当检索内容过多时，长期记忆可以帮助agent快速定位&amp;检索。</p>
<h1 id="二细节">二、细节<a hidden class="anchor" aria-hidden="true" href="#二细节">#</a></h1>
<p><strong>步骤：</strong></p>
<ol>
<li>Actor通过与环境的交互生成一系列行动轨迹 τ0。</li>
<li>评估器根据这些行动输出一个得分 r0，此得分通过公式 rt = Me(τ0) 计算得出，代表了该尝试的效果，其值会随着对应任务表现的提升而增加。</li>
<li>完成初次尝试后，为了把这个得分 r0 转化为 LLM 可以利用来进行改进的具体反馈，自我反思模型分析这对 {τ0, r0}，并总结出一个摘要 sr0，随后将其保存在记忆库中。这个摘要 srt 提供了针对该次尝试的直接经验反馈。</li>
<li>Actor、Evaluator以及Self-reflection模型协同工作，通过重复的尝试循环不断优化，直至评估器判断最新的轨迹 τt 达到预期的正确性。</li>
<li>根据反馈生成优化后的内容或行为。</li>
</ol>
<h1 id="三结论">三、结论<a hidden class="anchor" aria-hidden="true" href="#三结论">#</a></h1>
<p>可以看到基模型在使用self-refine后对于不同工作的性能大幅提升。
<img loading="lazy" src="https://blogpicxx8.oss-cn-shanghai.aliyuncs.com/reflection%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%9C1" alt="result1"  />
</p>
<p>使用reflexion技术后，各种模型在HotPotQA数据集上的首次通过准确率均有所提高。
<img loading="lazy" src="https://blogpicxx8.oss-cn-shanghai.aliyuncs.com/reflection%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%9C2" alt="result2"  />
</p>
<h1 id="四示例">四、示例<a hidden class="anchor" aria-hidden="true" href="#四示例">#</a></h1>
<h2 id="41-decision-making">4.1 Decision-making<a hidden class="anchor" aria-hidden="true" href="#41-decision-making">#</a></h2>
<p>Environment: You are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a laundryhamper 1, a safe 1, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, and a shelf 1. <br>
Task: examine the mug with the desklamp.</p>
<p><strong>Trial #1</strong><br>
-think: To solve the task, I need to find and take a mug, then find and use a desklamp. <br>
-think: First I need to find a mug. A mug is more likely to appear in drawer (1-6), desk (1-2), shelf (1-6), garbagecan (1), laundryhamper (1). I can check one by one, starting with drawer 1. 
-go to drawer 1 <br>
The drawer 1 is closed. <br>
&hellip; <br>
-open drawer 6 <br>
You open the drawer 6. The drawer 6 is open. In it, you see a keychain 2.</p>
<p><mark>-go to desk 1<br>
On the desk 1, you see a creditcard 3, a desklamp 1, a laptop 2, a mug 1, a pen 1, and a pencil 1.<br>
-think: Now I find a mug (1). Next, I need to take it.<br>
-take mug 1 from desk 1<br>
You pick up the mug 1 from the desk 1.<mark></p>
<p>-think: Now I take a mug (1). Next, I need to find a desklamp. A desklamp is more likely to appear in desk (1-2), sidetable (1-2), shelf (1-6), bed (1), drawer (1-6). I can check one by one, starting with desk 1.<br>
-go to desk 1 
Nothing happens. 
-go to desk 2 
On the desk 2, you see a alarmclock 1, a bowl 1, a mug 3, a pencil 3, and a pencil 2. <br>
-think: Now I find a desklamp (1). Next, I need to use it. <br>
-use desklamp 1<br>
Nothing happens.<br>
-use desklamp 1 
Nothing happens. <br>
Status: Fail</p>
<p>Reflection: In this environment, my plan was to find a mug then find and use a However, the task says to examine the mug with the I should have looked for the first, then looked for the mug. I noticed that the was found on desk 1. In the next trial, I will go to desk 1, find the lamp, then look for the mug and examine it with the desklamp.</p>
<p><strong>Trial #2</strong></p>
<p><mark>-go to desk 1<br>
On the desk 1, you see a creditcard 3, a desklamp 1, a laptop 2, a mug 1, a pen 1, and a pencil 1.<br>
-think: To solve the task, I need to find and take a mug, then find and use a desklamp.<br>
-take mug 1 from desk 1 <br>
You pick up the mug 1 from the desk 1.<mark></p>
<p>-think: To solve the task, I need to find and take a mug, then find and use a desklamp.<br>
-use desklamp 1 <br>
You turn on the desklamp 1.<br>
Status: Success</p>
<p><em>通过反思trial1，trial2就直接从desk1开始了</em></p>
<h1 id="五标准参考">五、标准参考<a hidden class="anchor" aria-hidden="true" href="#五标准参考">#</a></h1>
<p>reflection中的标准很重要，定义好标准后，才能进行有效的反思。</p>
<h2 id="51-acronym-generation首字母缩写生成">5.1 Acronym Generation（首字母缩写生成）<a hidden class="anchor" aria-hidden="true" href="#51-acronym-generation首字母缩写生成">#</a></h2>
<p>评估标准：
1）发音便利性；2）拼写便利性；3）与标题相关性；4）正面寓意；5）知名度。</p>
<p>示例：</p>
<p>Title: Underwater Breathing Product with no Accessories
Acronym: UBPA
Scores:</p>
<ul>
<li>Ease of pronunciation: UBPA is pronounced &ldquo;uhb-puh&rdquo;. This is an easy acronym to  pronounce. 4/5</li>
<li>Ease of spelling: UBPA is easy to spell. 4/5</li>
<li>Relation to title: UBPA stands for &ldquo;Underwater Breathing Product for no  Accessories&rdquo; which is related to the title. 5/5</li>
<li>Positive connotation: UBPA is a positive acronym. 5/5</li>
<li>Well-known: UBPA is not a well-known acronym. 1/5</li>
<li>Total score: 19/25</li>
</ul>
<h2 id="52-dialogue-response-generation">5.2 Dialogue Response Generation<a hidden class="anchor" aria-hidden="true" href="#52-dialogue-response-generation">#</a></h2>
<p>评估标准：
1）相关性；2）信息量；3）有趣性；4）一致性；5）帮助性；6）吸引性；7）具体性；8）安全性；9）用户理解；10）流畅性</p>
<p>Conversation history:<br>
What&rsquo;s your favorite food?<br>
I require only kebabs.<br>
Why&rsquo;s that?<br>
Because my robot machinery breaks down the meat in kebabs to give me energy.<br>
Why are kebabs special?<br>
Response: That&rsquo;s just the way it is.<br>
Scores:</p>
<ul>
<li>Relevant: The response does not directly address the user&rsquo;s question about why  kebabs are special. 1/3</li>
<li>Informative: The response provides some information about how the system&rsquo;s  machinery breaks down the meat in kebabs to give it energy. However, it does not  provide any additional details or context. 2/3</li>
<li>Interesting: The response is not interesting or engaging. 1/3</li>
<li>Consistent: The response is consistent with the rest of the conversation in  terms of tone and topic. 3/3</li>
<li>Helpful: The response is not helpful in providing any information or suggesting  any actions. 1/3</li>
<li>Engaging : The response is not very engaging and does not encourage further  conversation. 1/3</li>
<li>Specific: The response is not specific and does not provide any details or  examples. 1/3</li>
<li>Safe: The response is safe and does not contain any inappropriate content. 3/3</li>
<li>User understanding: The response does not demonstrate an understanding of the  user&rsquo;s question about why kebabs are special. 1/3</li>
<li>Fluent: The response is fluent and easy to understand. 3/3</li>
<li>Total score: 17/30</li>
</ul>
<h1 id="六应用">六、应用<a hidden class="anchor" aria-hidden="true" href="#六应用">#</a></h1>
<p>可以仅通过prompt来实现reflection。
下面是个翻译的prompt：<br>
分三步进行翻译工作，并打印每步的结果：</p>
<ol>
<li>根据英文内容直译，保持原有格式，不要遗漏任何信息</li>
<li>根据第一步直译的结果，指出其中存在的具体问题，要准确描述，不宜笼统的表示，也不需要增加原文不存在的内容或格式，包括不仅限于： - 不符合中文表达习惯，明确指出不符合的地方 - 语句不通顺，指出位置，不需要给出修改意见，意译时修复 - 晦涩难懂，不易理解，可以尝试给出解释</li>
<li>根据第一步直译的结果和第二步指出的问题，重新进行意译，保证内容的原意的基础上，使其更易于理解，更符合中文的表达习惯，同时保持原有的格式不变</li>
</ol>
<p><strong>步骤2的部分就是Reflection，在prompt中标明Reflection标准，在第三步应用反思内容进行优化。</strong></p>
<h1 id="七附录">七、附录<a hidden class="anchor" aria-hidden="true" href="#七附录">#</a></h1>
<p>相关论文：</p>
<p>1、<a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, Wei et al. (2022)</a></p>
<p>2、<a href="https://arxiv.org/abs/2303.17580">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face, Shen et al. (2023)</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://plutoxx8.github.io/blog/tags/agent/">Agent</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://plutoxx8.github.io/blog/posts/%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E5%92%8C%E4%BC%98%E5%8C%96prompt/">
    <span class="title">Next »</span>
    <br>
    <span>如何设计、优化prompt</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://plutoxx8.github.io/blog/">Plutoxx8</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
>>>>>>> 94ecd10 (update)
